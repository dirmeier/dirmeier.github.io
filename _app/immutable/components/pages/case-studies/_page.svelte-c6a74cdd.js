import{S as W,i as Y,s as Z,k,q as x,a as I,l as v,m as y,r as B,h as d,c as S,b as E,G as u,B as N,K as R,e as F,n as T,I as ee,p as te,w as V,J as ie,x as H,y as q,f as G,t as J,z as O}from"../../../chunks/index-21de0154.js";import{N as ne}from"../../../chunks/_navbar-8c0527b3.js";import{B as ae}from"../../../chunks/_bottom-031b3214.js";const se=[{date:"June 2022",title:"Diffusion models I",description:"In this case study we cover the basics of diffusion probabilistic models. The notebook uses Distrax and Haiku for probabilistic inference",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/etudes/diffusion_models.html"}]},{date:"January 2022",title:"Normalizing flows for variational inference",description:"This case study implements an inverse autoregressive flow for variational inference of parameters. The notebook uses Distrax and Haiku for probabilistic inference",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/etudes/normalizing_flows_for_vi.html"}]}],oe=[{date:"September 2021",title:"Causal inference using tensor-product smoothing splines",description:"The case study reproduces and improves upon a probabilistic model for causal inference with structured latent confounders. It uses BlackJAX, NumPyro and Stan for probabilistic inference",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/etudes/causal_inference_using_tensor_product_smoothing_splines.html"}]},{date:"August 2021",title:"Stick-breaking constructions and variational inference",description:"Here we implement common stick-breaking constructions for mean-field variational inference in nonparametric mixture and factor models. The notebook uses NumPyro and TFP for probabilistic inference",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/etudes/stick_breaking_constructions.html"}]},{date:"July 2021",title:"Hilbert-space approximate copula processes",description:"In this notebook we explore the application of Hilbert-space methods for the approximation of Gaussian copula processes to model stochastic volatility. The notebook uses Stan for probabilistic inference",language:"R",links:[{name:"link",link:"https://dirmeier.github.io/etudes/low_rank_copula_processes.html"}]},{date:"June 2021",title:"Variational, multivariate LSTMs",description:"LSTMs provide an intriguing approach to timeseries forecasting since they naturally model their temporal dependencies. In this notebook we test implementing a variational, multivariate LSTM using Haiku and Numpyro to predict US election outcomes",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/etudes/variational_lstms.html"}]},{date:"May 2021",title:"Hierarchical, coregionalized GPs",description:"Hierarchical and coregionalized GPs are two approaches to modelling marginally correlated data. In this notebook we implement two GP models using Numpyro and compare their predictive performance as well as MCMC diagnostics on an US election data set",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/etudes/gp_coregionalization.html"}]},{date:"March 2021",title:"The basics of Bayesian optimization",description:"Bayesian optimization provides a unified framework to optimization of costly-to-evaluate objective functions. In this notebooks we demonstrate the basics of BO using Stan",language:"R",links:[{name:"link",link:"https://dirmeier.github.io/etudes/bayesian_optimization.html"}]}],le=[{date:"October 2020",title:"Normalizing flows for density estimation",description:"Using TensorFlow Probability's Bijector API one can easily implement custom normalizing flows. This notebook shows how it can be done using MAFs as an example",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/etudes/normalizing_flows.html"}]},{date:"June 2020",title:"Causal structure learning with VAEs",description:"Learning cause-effect mechanisms among a set of random variables is not only of great epistemological interest but also a fascinating statistical problem. In this notebook we implement a graph variational autoencoder to learn the DAG of a structural equations model and compare it to greedy equivalent search",language:"R",links:[{name:"link",link:"https://dirmeier.github.io/etudes/causal_structure_learning.html"}]},{date:"May 2020",title:"On sequential regression models",description:"Sequential models are a special type of ordinal regression models, but additionally assume that categories can only be reached sequentially. This case study shows the difference to conventional ordinal models",language:"R",links:[{name:"link",link:"https://dirmeier.github.io/rstansequential/index.html"}]}],re=[{date:"October 2019",title:"Mixed model reference implementations",description:"Concise reference implementations to fit (generalized) linear mixed models in Python",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/mixed-models/index.html"}]},{date:"September 2019",title:"Structure learning for Bayesian networks",description:"A Python notebook on structure MCMC to learn the structure of a Bayesian network using PyMC3",language:"Python",links:[{name:"link",link:"https://dirmeier.github.io/structure-learning-with-pymc/index.html"}]},{date:"June 2019",title:"Simulation-based calibration",description:"An example notebook for simulation-based calibration for validation of Bayesian inferences",language:"R",links:[{name:"link",link:"https://dirmeier.github.io/etudes/simulation_based_calibration.html"}]},{date:"January 2019",title:"Dirichlet process mixture models",description:"Stan models on infinite Bayesian mixtures using the Dirichlet process",language:"R",links:[{name:"link",link:"https://dirmeier.github.io/etudes/dirichlet_process_mixture_models.html"}]}],ce=[{date:"October 2018",title:"Philosophy of Science",description:"Some references on philosophy of science that are in my opinion worth reading",links:[{name:"link",link:"https://dirmeier.github.io/on-philosophy-of-science/index.html"}]},{date:"October 2018",title:"Gaussian Processes",description:"Bayesian non-parametrics such as Gaussian Processes are a wonderful approach to machine learning. If you are also an enthusiast make sure to check out my Python notebooks on regression and classification",language:"R",links:[{name:"regression",link:"https://dirmeier.github.io/etudes/gaussian_process_regression.html"},{name:"classification",link:"https://dirmeier.github.io/etudes/gaussian_process_classification.html"}]}];function L(r,e,n){const s=r.slice();return s[1]=e[n],s}function U(r,e,n){const s=r.slice();return s[4]=e[n],s[6]=n,s}function j(r,e,n){const s=r.slice();return s[7]=e[n],s[9]=n,s}function ue(r){let e,n;return{c(){e=k("img"),this.h()},l(s){e=v(s,"IMG",{src:!0,width:!0,height:!0}),this.h()},h(){ee(e.src,n=he(r[4].language))||T(e,"src",n),T(e,"width","20"),T(e,"height","20")},m(s,h){E(s,e,h)},p:N,d(s){s&&d(e)}}}function de(r){let e,n;return{c(){e=k("span"),n=x(",")},l(s){e=v(s,"SPAN",{});var h=y(e);n=B(h,","),h.forEach(d)},m(s,h){E(s,e,h),u(e,n)},d(s){s&&d(e)}}}function K(r){let e,n,s,h=r[7].name+"",f,c,m,l,t=r[9]+1<r[4].links.length&&de();return{c(){e=x("("),n=k("span"),s=k("a"),f=x(h),c=I(),t&&t.c(),m=I(),l=x(")"),this.h()},l(i){e=B(i,"("),n=v(i,"SPAN",{});var a=y(n);s=v(a,"A",{href:!0,style:!0});var o=y(s);f=B(o,h),o.forEach(d),c=S(a),t&&t.l(a),m=S(a),a.forEach(d),l=B(i,")"),this.h()},h(){T(s,"href",r[7].link),te(s,"display","inline")},m(i,a){E(i,e,a),E(i,n,a),u(n,s),u(s,f),u(n,c),t&&t.m(n,null),u(n,m),E(i,l,a)},p:N,d(i){i&&d(e),i&&d(n),t&&t.d(),i&&d(l)}}}function X(r){let e,n,s=r[4].date+"",h,f,c,m,l=r[4].title+"",t,i,a,o=r[4].description+"",w,p,M,P=r[4].language&&ue(r),D=r[4].links,b=[];for(let _=0;_<D.length;_+=1)b[_]=K(j(r,D,_));return{c(){e=k("div"),n=k("div"),h=x(s),f=I(),c=k("div"),m=k("h6"),t=x(l),i=I(),P&&P.c(),a=I(),w=x(o),p=I();for(let _=0;_<b.length;_+=1)b[_].c();M=I(),this.h()},l(_){e=v(_,"DIV",{class:!0});var $=y(e);n=v($,"DIV",{class:!0});var g=y(n);h=B(g,s),g.forEach(d),f=S($),c=v($,"DIV",{class:!0});var z=y(c);m=v(z,"H6",{});var C=y(m);t=B(C,l),i=S(C),P&&P.l(C),C.forEach(d),a=S(z),w=B(z,o),p=S(z);for(let A=0;A<b.length;A+=1)b[A].l(z);z.forEach(d),M=S($),$.forEach(d),this.h()},h(){T(n,"class","column is-3"),T(c,"class","column is-9"),T(e,"class","columns")},m(_,$){E(_,e,$),u(e,n),u(n,h),u(e,f),u(e,c),u(c,m),u(m,t),u(m,i),P&&P.m(m,null),u(c,a),u(c,w),u(c,p);for(let g=0;g<b.length;g+=1)b[g].m(c,null);u(e,M)},p(_,$){if(_[4].language&&P.p(_,$),$&1){D=_[4].links;let g;for(g=0;g<D.length;g+=1){const z=j(_,D,g);b[g]?b[g].p(z,$):(b[g]=K(z),b[g].c(),b[g].m(c,null))}for(;g<b.length;g+=1)b[g].d(1);b.length=D.length}},d(_){_&&d(e),P&&P.d(),R(b,_)}}}function Q(r){let e,n,s=r[1].date+"",h,f,c,m=r[1].posts,l=[];for(let t=0;t<m.length;t+=1)l[t]=X(U(r,m,t));return{c(){e=k("div"),n=k("h4"),h=x(s),f=I();for(let t=0;t<l.length;t+=1)l[t].c();c=F()},l(t){e=v(t,"DIV",{});var i=y(e);n=v(i,"H4",{});var a=y(n);h=B(a,s),a.forEach(d),i.forEach(d),f=S(t);for(let o=0;o<l.length;o+=1)l[o].l(t);c=F()},m(t,i){E(t,e,i),u(e,n),u(n,h),E(t,f,i);for(let a=0;a<l.length;a+=1)l[a].m(t,i);E(t,c,i)},p(t,i){if(i&1){m=t[1].posts;let a;for(a=0;a<m.length;a+=1){const o=U(t,m,a);l[a]?l[a].p(o,i):(l[a]=X(o),l[a].c(),l[a].m(c.parentNode,c))}for(;a<l.length;a+=1)l[a].d(1);l.length=m.length}},d(t){t&&d(e),t&&d(f),R(l,t),t&&d(c)}}}function me(r){let e,n,s,h,f,c,m,l=r[0],t=[];for(let i=0;i<l.length;i+=1)t[i]=Q(L(r,l,i));return{c(){e=k("div"),n=k("h2"),s=x("Case studies"),h=I(),f=k("p"),c=x(`Below you can find a collection of case studies and notebooks on topics I work on or am
		interested in. Topics revolve generally around probabilistic modelling, ranging from
		probabilistic machine and deep learning, to Bayesian statistics, to causal inference. The case
		studies mainly use the probabilistic programming languages Stan, TensorFlow Probability and
		NumPyro.`),m=I();for(let i=0;i<t.length;i+=1)t[i].c()},l(i){e=v(i,"DIV",{});var a=y(e);n=v(a,"H2",{});var o=y(n);s=B(o,"Case studies"),o.forEach(d),h=S(a),f=v(a,"P",{});var w=y(f);c=B(w,`Below you can find a collection of case studies and notebooks on topics I work on or am
		interested in. Topics revolve generally around probabilistic modelling, ranging from
		probabilistic machine and deep learning, to Bayesian statistics, to causal inference. The case
		studies mainly use the probabilistic programming languages Stan, TensorFlow Probability and
		NumPyro.`),w.forEach(d),m=S(a);for(let p=0;p<t.length;p+=1)t[p].l(a);a.forEach(d)},m(i,a){E(i,e,a),u(e,n),u(n,s),u(e,h),u(e,f),u(f,c),u(e,m);for(let o=0;o<t.length;o+=1)t[o].m(e,null)},p(i,[a]){if(a&1){l=i[0];let o;for(o=0;o<l.length;o+=1){const w=L(i,l,o);t[o]?t[o].p(w,a):(t[o]=Q(w),t[o].c(),t[o].m(e,null))}for(;o<t.length;o+=1)t[o].d(1);t.length=l.length}},i:N,o:N,d(i){i&&d(e),R(t,i)}}}function he(r){return"/logos/"+r+"_logo.svg"}function fe(r){return[[{date:"2022",posts:se},{date:"2021",posts:oe},{date:"2020",posts:le},{date:"2019",posts:re},{date:"2018",posts:ce}]]}class pe extends W{constructor(e){super(),Y(this,e,fe,me,Z,{})}}function ge(r){let e,n,s,h,f,c,m,l,t,i,a,o,w;return s=new ne({}),i=new pe({}),o=new ae({}),{c(){e=I(),n=k("div"),V(s.$$.fragment),h=I(),f=k("div"),c=k("section"),m=k("div"),l=k("div"),t=k("div"),V(i.$$.fragment),a=I(),V(o.$$.fragment),this.h()},l(p){ie("svelte-1ine71f",document.head).forEach(d),e=S(p),n=v(p,"DIV",{});var P=y(n);H(s.$$.fragment,P),h=S(P),f=v(P,"DIV",{});var D=y(f);c=v(D,"SECTION",{class:!0});var b=y(c);m=v(b,"DIV",{class:!0});var _=y(m);l=v(_,"DIV",{class:!0});var $=y(l);t=v($,"DIV",{class:!0});var g=y(t);H(i.$$.fragment,g),a=S(g),H(o.$$.fragment,g),g.forEach(d),$.forEach(d),_.forEach(d),b.forEach(d),D.forEach(d),P.forEach(d),this.h()},h(){document.title="About",T(t,"class","column is-offset-4 is-6"),T(l,"class","columns"),T(m,"class","container"),T(c,"class","section")},m(p,M){E(p,e,M),E(p,n,M),q(s,n,null),u(n,h),u(n,f),u(f,c),u(c,m),u(m,l),u(l,t),q(i,t,null),u(t,a),q(o,t,null),w=!0},p:N,i(p){w||(G(s.$$.fragment,p),G(i.$$.fragment,p),G(o.$$.fragment,p),w=!0)},o(p){J(s.$$.fragment,p),J(i.$$.fragment,p),J(o.$$.fragment,p),w=!1},d(p){p&&d(e),p&&d(n),O(s),O(i),O(o)}}}class ve extends W{constructor(e){super(),Y(this,e,null,ge,Z,{})}}export{ve as default};
